# Document Processing Investigation - Final Summary

**Date**: 2026-02-02 22:10
**Status**: ðŸ” **ROOT CAUSE CONFIRMED**
**Severity**: ðŸš¨ **CRITICAL - Production Feature Not Working**

---

## TL;DR

**The problem**: 180 documents uploaded but 0 AI extractions

**The cause**: Infrastructure migration from Celery to Lambda was APPROVED but NEVER COMPLETED

**The proof**:
- âœ… Real extraction code EXISTS and is production-ready (758 lines, full AWS Bedrock integration)
- âœ… Lambda infrastructure EXISTS and is deployed
- âŒ Lambda worker calls MOCK stub instead of real code
- âŒ Migration Phase 3 "Update Worker Tasks" was never implemented

**The fix**: Connect Lambda worker to real processing code (2-3 weeks)

---

## Complete Timeline

### Before Dec 21, 2025: Working System

**Status**: Celery-based document processing functional

**Evidence**:
- Production code exists: `apps/worker-tasks/src/tasks/document_processing/process_document_task.py`
- Session file (Jan 8, 2026): "Golden path test passes: upload â†’ classify â†’ extract â†’ review_pending"
- Session file (Jan 8, 2026): "Database status: review_pending (8 fields extracted)"

**Functionality**:
- AWS Bedrock Claude classification
- Composite parsing (multiple strategies)
- ExtractionResult creation
- Auto-credential creation (code ready, MVP disabled)
- Pattern detection
- Validation
- **WORKING in local dev**

### Dec 20-21, 2025: Migration Decision

**ADR-006 Created**: "Lambda-Only Infrastructure Migration"

**Decision**: Migrate from EC2 + Celery â†’ Lambda-only
**Justification**:
- Cost savings: 18% reduction ($121/year)
- Simplified operations: single deployment model
- User directive: "Terminate any evidence of EC2"
- Future-proof: serverless best practices

**Implementation Plan** (from ADR-006):
```
Phase 1: Terraform Cleanup (2 hours)
Phase 2: Lambda Completion (3 hours)
Phase 3: Update Worker Tasks (3 hours)  â† NEVER DONE!
Phase 4: Testing & Deployment (2 hours)
Total: 8-10 hours
```

**Dec 21, 2025 Commit**:
```
feat: add Lambda infrastructure and session documentation

- Add Lambda function scaffolding for async document processing
- Document AWS cost optimization analysis and migration plan
```

**What was created**:
- Lambda function handler with SQS integration
- Mock `process_document_mock()` function
- TODO comments: "Replace with actual implementation"

**What was NOT done**:
- Phase 3: Update Worker Tasks
- Connect Lambda to real processing code
- Port Celery task logic to Lambda-compatible wrapper

### Jan 8-11, 2026: Production Documents Uploaded

**180 documents uploaded**:
- First: Jan 8, 2026 06:13
- Last: Jan 11, 2026 23:45
- All processed by mock Lambda worker
- Result: Basic classification only (5.8% confidence)
- **0 AI extractions**

**What users experienced**:
- Upload documents â†’ basic classification â†’ stuck at "review_pending"
- No field extraction
- No auto-populated credentials
- 100% manual data entry required

### Jan 8 - Feb 3, 2026: Multiple Lambda Deployments

**6+ Lambda deployments**:
- Jan 8: Container image deployment
- Jan 8: Zip deployment
- Jan 10: Infrastructure migration documented
- Jan 30: Deployment package complete
- Feb 1: Dev deployment
- Feb 3: Prod deployment (LATEST)

**Issue**: Every deployment still has mock code

**Observation**: Deployment pipeline working, but SOURCE CODE being deployed is stub

---

## The Critical Gap: Phase 3 Never Implemented

### ADR-006 Phase 3 (FROM ADR DOCUMENT)

**Quote from ADR**:
```
### Phase 3: Update Worker Tasks (3 hours)

**3.1 Remove Celery Dependencies**
- [ ] Update apps/worker-tasks/src/celery_app.py
  - Remove Celery broker/backend config
  - Replace with direct function calls
- [ ] Update process_document_task.py
  - Remove @celery_app.task decorator
  - Keep processing logic (AWS Bedrock + Textract)
  - Make callable from Lambda handler

**3.2 Create Lambda Wrapper**
File: infra/lambda/functions/worker/process_document.py
```python
from apps.worker_tasks.src.tasks.document_processing.process_document_task import process_document

def process_document_wrapper(document_id: str) -> Dict[str, Any]:
    \"\"\"Lambda wrapper for document processing task.\"\"\"
    # Initialize DB session
    # Call process_document (no Celery decorator)
    # Return result
```

**3.3 Update Dependencies**
- [ ] Add worker dependencies to Lambda requirements.txt
  - PyMuPDF (PDF parsing)
  - Tesseract (OCR)
  - boto3 (Bedrock, Textract)
  - All shared modules

**3.4 Update Deployment Package**
- [ ] Include apps/worker-tasks/src in Lambda zip/container
- [ ] Include apps/backend-api/src/shared (for enums, services)
- [ ] Include apps/backend-api/src/contexts (for validators)
```

**Status**: âŒ **NONE OF THIS WAS DONE**

---

## What Exists vs What's Deployed

### Real Implementation (EXISTS, NOT USED)

**File**: `apps/worker-tasks/src/tasks/document_processing/process_document_task.py`
**Lines**: 758
**Created**: Initial commit (before Dec 2025)
**Last Modified**: Jan 8, 2026 (admin upload tracking)

**Features** (ALL IMPLEMENTED):
```python
def process_document(document_id: str):
    # 1. Fetch document from DB
    document = db.query(WorkerDocument).filter_by(id=document_id).first()

    # 2. Download from S3
    file_bytes = s3_service.download_file(document.s3_key)

    # 3. Extract text (PyMuPDF for PDF, Tesseract for images)
    if document.content_type == "application/pdf":
        pdf_doc = fitz.open(stream=file_bytes)
        document_text = extract_text(pdf_doc)

    # 4. Classify document type (AWS Bedrock Claude)
    type_detector = DocumentTypeDetector()
    classification = type_detector.classify(file_bytes, document_text)

    # 5. Extract fields (CompositeParser: Textract + Bedrock)
    parser = CompositeParser(enable_bedrock=True)
    parse_result = parser.parse(document_id, document_type, file_bytes, document_text)

    # 6. Validate extracted data
    if document_type == 'cme_certificate':
        validator = CMEValidator()
        validation = validator.validate(extracted_data)

    # 7. Create ExtractionResult record
    extraction_result = WorkerExtractionResult(
        document_id=document.id,
        extraction_method="textract+bedrock",
        extracted_data=parse_result.data,
        extraction_confidence=parse_result.confidence,
        validation_passed=validation.passed
    )
    db.add(extraction_result)

    # 8. Pattern detection (accuracy tracking)
    pattern = pattern_detector.detect_pattern(document, extraction_result)
    extraction_result.scrutiny_level = determine_scrutiny_level(pattern)

    # 9. Auto-link to provider by NPI
    if extracted_data.get('provider_npi'):
        provider = find_provider_by_npi(extracted_data['provider_npi'])
        document.provider_id = provider.id

    # 10. Auto-create credentials (if confidence high enough)
    if can_auto_create_credential(confidence, validation_passed):
        auto_create_service.handle_extraction_result(extraction_result)

    # 11. Update document status
    document.status = "review_pending"
    db.commit()

    # 12. Send notification
    notification_service.notify_document_processed(user_id, document_id)

    return {"success": True, "extraction_id": extraction_result.id}
```

**Dependencies**:
- AWS Bedrock (boto3)
- AWS Textract (optional, code ready)
- PyMuPDF (PDF parsing)
- Tesseract (image OCR)
- Celery (for task decorator)

### Mock Implementation (DEPLOYED TO PRODUCTION)

**File**: `infra/lambda/functions/worker/handler.py`
**Lines**: 177
**Created**: Dec 21, 2025
**Last Deployed**: Feb 3, 2026 00:23

**Features** (NONE):
```python
def process_document_mock(document_id: str) -> Dict[str, Any]:
    """
    Mock implementation for testing (will be replaced with actual logic).
    """
    logger.info(f"Processing document: {document_id}")

    # TODO: Replace with actual implementation
    # from tasks.document_processing import process_document_internal
    # return process_document_internal(document_id)

    return {
        "status": "success",
        "document_id": document_id,
        "message": "Mock processing completed"
    }
```

**Dependencies**: None (just logs and returns mock success)

---

## Why This Happened

### Root Cause Analysis

**Proximate Cause**: Phase 3 of ADR-006 implementation never executed

**Contributing Factors**:

1. **Incomplete Migration Planning**
   - ADR-006 estimated 8-10 hours total
   - Phase 3 (most complex) only allocated 3 hours
   - Underestimated porting Celery code to Lambda wrapper

2. **Testing Gap**
   - Lambda deployed without end-to-end testing
   - Golden pathway test exists but wasn't run against Lambda
   - Mock function "succeeds" so deployment appeared healthy

3. **Deployment Process**
   - SAM deployment successful (infrastructure working)
   - Source code validation missing (no check for mock vs real)
   - Multiple deployments without fixing root issue

4. **Environment Confusion**
   - Local dev uses Celery (working)
   - Production uses Lambda (mock)
   - Different code paths in different environments
   - Testing in local didn't catch production issue

5. **Silent Failure**
   - Mock function returns `{"status": "success"}`
   - No error thrown, no alert triggered
   - Documents appear "processed" (basic classification happens)
   - Users experience degraded service, not outage

### What Should Have Happened

**After Dec 21 Lambda scaffold created**:
1. âœ… Create Lambda infrastructure (DONE)
2. âœ… Deploy mock for testing (DONE)
3. âŒ Port real processing code (SKIPPED!)
4. âŒ Test end-to-end (SKIPPED!)
5. âŒ Verify extractions working (SKIPPED!)
6. âœ… Deploy to production (DONE - but with mock!)

**Before first production use (Jan 8)**:
- Verify 180 documents â†’ 180 extractions
- Verify extraction_results table populated
- Verify auto-credential creation working
- Compare production vs local behavior

---

## Business Impact

### Quantitative

**Documents Affected**: 180
**Processing Attempts**: 180
**Successful Extractions**: 0
**Success Rate**: 0%

**User Effort**:
- Manual data entry: 100% (should be 0-30%)
- Review time: 3-5x longer than auto-populated
- Error rate: Higher (manual entry vs AI extraction)

**Cost**:
- 180 Lambda invocations: ~$0.001 (negligible)
- Lost productivity: ~15-30 hours of manual entry
- User frustration: High (key feature not working)

### Qualitative

**User Experience**:
- Upload document â†’ basic classification â†’ manual entry
- No AI assistance despite AWS Bedrock cost
- Core product differentiator not working

**Technical Debt**:
- Two code paths (Celery local, Lambda prod)
- Incomplete migration
- Mock code in production
- Testing gap

**Reputation Risk**:
- AI extraction promised but not delivered
- Users paying for feature that doesn't work

---

## The Fix (Detailed)

### Immediate Action (2-3 Weeks)

**Step 1: Create Lambda Wrapper** (4 hours)

File: `infra/lambda/functions/worker/process_document_wrapper.py`

```python
"""
Lambda wrapper for real document processing.
Bridges SQS Lambda event â†’ Celery task logic.
"""
import os
import uuid
import logging
from typing import Dict, Any
from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker

# Import real processing logic (no Celery decorator)
import sys
sys.path.insert(0, '/opt/worker-tasks')
sys.path.insert(0, '/opt/shared')

# The real implementation (Celery decorator removed for Lambda)
from tasks.document_processing.core import process_document_core

logger = logging.getLogger(__name__)

# Database setup
DATABASE_URL = os.getenv("DATABASE_URL")
engine = create_engine(DATABASE_URL, pool_pre_ping=True)
SessionLocal = sessionmaker(bind=engine)

def lambda_process_document(document_id: str) -> Dict[str, Any]:
    """
    Lambda entry point - calls real processing logic.

    This replaces process_document_mock() in handler.py.
    """
    db = SessionLocal()
    try:
        logger.info(f"[Lambda] Processing document {document_id}")

        # Call real processing (same as Celery task but no decorator)
        result = process_document_core(
            db=db,
            document_id=document_id,
            enable_bedrock=True  # Lambda has AWS credentials
        )

        logger.info(f"[Lambda] Processing complete: {result['extraction_id']}")
        return result

    except Exception as e:
        logger.error(f"[Lambda] Processing failed: {e}", exc_info=True)
        raise  # Let Lambda retry via SQS

    finally:
        db.close()
```

**Step 2: Refactor Celery Task** (2 hours)

File: `apps/worker-tasks/src/tasks/document_processing/core.py` (new file)

```python
"""
Core document processing logic (Celery-independent).
Can be called from Celery task or Lambda function.
"""

def process_document_core(
    db: Session,
    document_id: str,
    enable_bedrock: bool = True
) -> Dict[str, Any]:
    """
    Core processing logic - no Celery dependencies.

    Args:
        db: Database session
        document_id: UUID of document
        enable_bedrock: Enable AWS Bedrock for classification

    Returns:
        Processing result dict
    """
    # (Move all 758 lines of logic from process_document() here)
    # No @celery_app.task decorator
    # Same exact logic, just standalone function
    ...
```

File: `apps/worker-tasks/src/tasks/document_processing/process_document_task.py` (update)

```python
"""Celery task wrapper (for local dev only)."""
from celery import Task
from src.celery_app import celery_app
from .core import process_document_core

class DocumentProcessingTask(Task):
    autoretry_for = (Exception,)
    retry_kwargs = {'max_retries': 3}

@celery_app.task(base=DocumentProcessingTask, bind=True)
def process_document(self, document_id: str):
    """Celery wrapper - calls core logic."""
    db = SessionLocal()
    try:
        return process_document_core(db, document_id, enable_bedrock=False)
    finally:
        db.close()
```

**Step 3: Update Lambda Deployment** (3 hours)

**3.1 Update handler.py**:

```python
# Replace this:
result = process_document_mock(document_id)

# With this:
from process_document_wrapper import lambda_process_document
result = lambda_process_document(document_id)
```

**3.2 Create Lambda layers**:

```bash
# Layer 1: Worker code
cd apps/worker-tasks
zip -r /tmp/worker-layer.zip src/

# Layer 2: Shared modules
cd apps/backend-api
zip -r /tmp/shared-layer.zip src/shared/ src/contexts/

# Layer 3: Dependencies
pip install PyMuPDF pytesseract boto3 -t /tmp/python/lib/python3.11/site-packages/
cd /tmp && zip -r deps-layer.zip python/
```

**3.3 Update SAM template**:

```yaml
WorkerFunction:
  Type: AWS::Serverless::Function
  Properties:
    Handler: handler.lambda_handler  # Updated to call real code
    Layers:
      - !Ref WorkerCodeLayer
      - !Ref SharedModulesLayer
      - !Ref DependenciesLayer
    Environment:
      Variables:
        ENABLE_BEDROCK: "true"
        DATABASE_URL: !Sub "{{resolve:secretsmanager:${CombinedSecretArn}:SecretString:database_url}}"
```

**Step 4: Test Locally** (2 hours)

```bash
# 1. Deploy to dev environment
cd infra/lambda
sam build
sam deploy --stack-name credmate-lambda-dev --parameter-overrides Environment=dev

# 2. Upload test document via API
curl -X POST https://api-dev.credentialmate.com/documents/upload \
  -H "Authorization: Bearer $TOKEN" \
  -F "file=@test-license.pdf"

# 3. Verify extraction_results table populated
psql $DATABASE_URL -c "SELECT * FROM extraction_results ORDER BY created_at DESC LIMIT 1;"

# Expected: 1 row with extracted_data, confidence > 0.8, validation_passed = true

# 4. Verify document status
psql $DATABASE_URL -c "SELECT id, status, document_type FROM documents ORDER BY created_at DESC LIMIT 1;"

# Expected: status = 'review_pending', document_type = 'license'
```

**Step 5: Reprocess 180 Documents** (1 hour)

```bash
# Send all 180 documents back through queue
psql $DATABASE_URL -c "
SELECT id FROM documents
WHERE created_at BETWEEN '2026-01-08' AND '2026-01-12'
" | while read doc_id; do
  aws sqs send-message \
    --queue-url https://sqs.us-east-1.amazonaws.com/account/credmate-worker-queue \
    --message-body "{\"document_id\": \"$doc_id\"}"
done

# Wait 30-60 minutes for processing

# Verify results
psql $DATABASE_URL -c "
SELECT
  COUNT(*) as total,
  COUNT(DISTINCT er.id) as extractions_created,
  AVG(er.extraction_confidence) as avg_confidence
FROM documents d
LEFT JOIN extraction_results er ON d.id = er.document_id
WHERE d.created_at BETWEEN '2026-01-08' AND '2026-01-12'
"

# Expected:
# total: 180
# extractions_created: 180
# avg_confidence: > 0.80
```

**Step 6: Deploy to Production** (1 hour)

```bash
# Deploy to prod
sam deploy --stack-name credmate-lambda-prod --parameter-overrides Environment=prod

# Monitor CloudWatch logs
aws logs tail /aws/lambda/credmate-lambda-prod-WorkerFunction --follow

# Verify metrics
aws cloudwatch get-metric-statistics \
  --namespace AWS/Lambda \
  --metric-name Invocations \
  --dimensions Name=FunctionName,Value=credmate-lambda-prod-WorkerFunction \
  --start-time 2026-02-03T00:00:00Z \
  --end-time 2026-02-03T23:59:59Z \
  --period 3600 \
  --statistics Sum
```

---

## Success Criteria

**âœ… Fix Complete When**:

1. **Extraction Working**:
   - [ ] Lambda calls real `process_document_core()` (not mock)
   - [ ] extraction_results table populated (180 rows)
   - [ ] document_classifications table populated (180 rows)
   - [ ] Average extraction confidence >80%

2. **Auto-Create Working**:
   - [ ] 100-140 credentials auto-created (70-80% validation pass rate)
   - [ ] Users reviewing auto-populated data (not entering from scratch)
   - [ ] Auto-link to providers by NPI working

3. **Production Metrics**:
   - [ ] Lambda success rate >95%
   - [ ] Average processing time <30 seconds
   - [ ] CloudWatch logs show "Extraction complete" (not "Mock processing")
   - [ ] No errors in DLQ (dead letter queue)

4. **User Experience**:
   - [ ] Upload â†’ extract â†’ review workflow <2 minutes
   - [ ] Users report data pre-filled
   - [ ] Manual data entry time reduced 70%+

---

## Lessons Learned

### What Went Wrong

1. **Incomplete Migration**
   - ADR approved, Phase 1-2 done, Phase 3 skipped
   - Deployment before implementation complete

2. **Testing Gap**
   - No end-to-end test in production environment
   - Mock function "succeeds" so no alert

3. **Silent Failure**
   - Documents appear processed (basic classification)
   - No error, just missing feature
   - Users experience degradation, not outage

4. **Environment Divergence**
   - Local dev (Celery) works
   - Production (Lambda) doesn't
   - Different code paths not tested together

### How to Prevent

1. **Phase Completion Gates**
   - Don't deploy Phase N+1 until Phase N verified
   - Checklist: "Phase 3 complete? Y/N"

2. **End-to-End Testing**
   - Test full workflow in target environment
   - Verify extraction_results table populated
   - Compare local vs production behavior

3. **Smoke Tests**
   - After deployment, upload test document
   - Verify extraction_results created
   - Alert if table count doesn't increase

4. **Code Linting for Mocks**
   - Flag `process_document_mock` in production code
   - Pre-commit hook: "Mock functions not allowed in prod"

5. **Deployment Validation**
   - SAM deploy success != feature working
   - Validate: upload doc â†’ check extraction_results table
   - Rollback if extraction count = 0

---

## Complete Reports

1. **Document Processing Investigation**: `20260202-2145-document-processing-investigation.md`
   - Full analysis of why 180 documents aren't being processed
   - Code examples, implementation recommendations

2. **Worker Git History Analysis**: `20260202-2200-worker-git-history-analysis.md`
   - Timeline reconstruction
   - Commit history analysis
   - ADR-006 discovery

3. **Schema Drift Analysis**: `20260202-2030-schema-drift-analysis.md`
   - 12 missing tables in production
   - 16 pending migrations
   - Schema comparison

4. **Data Volume Comparison**: `20260202-2040-data-volume-comparison.md`
   - 18,044 rows in production vs 569 in local
   - 31.7x more data in production
   - Business metrics analysis

---

## Conclusion

**Status**: âœ… **ROOT CAUSE CONFIRMED**

**The Problem**:
- 180 documents uploaded to production
- 0 AI extractions performed
- Users doing 100% manual data entry

**The Cause**:
- Infrastructure migration from Celery â†’ Lambda was APPROVED (ADR-006)
- Phase 1-2 completed (Terraform cleanup, Lambda deployment)
- Phase 3 SKIPPED (Update Worker Tasks - connect real code to Lambda)
- Mock stub deployed to production instead of real processing logic

**The Evidence**:
- Real code exists: 758 lines, production-ready, full AWS Bedrock integration
- Lambda exists: Deployed 6+ times, always with mock stub
- ADR-006 documents the plan: Phase 3 never implemented
- Session files confirm: Local dev worked (Celery), production doesn't (Lambda mock)

**The Fix**: 2-3 weeks
1. Create Lambda wrapper calling real processing code
2. Refactor Celery task to core logic (no decorator)
3. Update Lambda deployment to include worker code
4. Test locally
5. Reprocess 180 documents
6. Deploy to production

**Next Action**: Implement Phase 3 of ADR-006 (the missing piece)

---

**Report Generated**: 2026-02-02 22:10 UTC
**Investigation Duration**: 3 hours
**Confidence**: 100% (root cause confirmed in code, ADR, and git history)
